# æŒ‘æˆ°ï¼šå¯¦ä½œ MCP Sampling

## ç›®å‰é€²åº¦ âœ…

æ­å–œï¼ä½ å·²ç¶“å®Œæˆäº†ï¼š
- å»ºç«‹ç¬¬ä¸€å€‹ MCP Server
- å¯¦ä½œ RAG å‘é‡æœå°‹åŠŸèƒ½

ç¾åœ¨è®“æˆ‘å€‘é€²å…¥ Sampling æŒ‘æˆ°ï¼ğŸš€

---

## ä»€éº¼æ˜¯ Samplingï¼ŸğŸ¤”

åœ¨ä¹‹å‰çš„æŒ‘æˆ°ä¸­ï¼Œéƒ½æ˜¯ **AI å‘¼å« Server çš„å·¥å…·**ã€‚ä½† Sampling è®“æˆ‘å€‘å¯ä»¥åéä¾†ï¼š

> **Server è«‹æ±‚ AI å¹«å¿™ç”Ÿæˆå…§å®¹ï¼**

é€™é–‹å•Ÿäº†è¨±å¤šå¯èƒ½æ€§ï¼š
- è®“ Server è«‹æ±‚ AI ç¸½çµæœå°‹çµæœ
- è®“ Server è«‹æ±‚ AI ç¿»è­¯å…§å®¹
- å¤šæ­¥é©Ÿå·¥ä½œæµç¨‹ä¸­çš„ AI è¼”åŠ©
- è‡ªå‹•åŒ–å…§å®¹ç”Ÿæˆç®¡ç·š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚  Tool   â”‚                 â”‚
â”‚   AI Client     â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   MCP Server    â”‚
â”‚  (Cursor/Claude)â”‚         â”‚                 â”‚
â”‚                 â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚                 â”‚
â”‚                 â”‚ Samplingâ”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æŒ‘æˆ°ç›®æ¨™ ğŸ¯

å¯¦ä½œä¸€å€‹å…·å‚™ **Sampling** åŠŸèƒ½çš„ MCP Serverï¼Œè®“ Server å¯ä»¥è«‹æ±‚ AI å¹«å¿™ï¼š
1. ç¸½çµæ–°èæ–‡ç« 
2. ç¿»è­¯å…§å®¹
3. ç”Ÿæˆæ™ºæ…§å›æ‡‰

---

## Step 1: å»ºç«‹ Sampling Server

å‰µå»ºä¸€å€‹æ–°æª”æ¡ˆ `sampling_server.py`ï¼š

```python
# sampling_server.py
from fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base
from mcp.types import TextContent

# 1. åˆå§‹åŒ– MCP Serverï¼ˆå•Ÿç”¨ samplingï¼‰
mcp = FastMCP("Sampling Demo Server", host="localhost", port=8080)

# æ¨¡æ“¬çš„æ–°èè³‡æ–™åº«
news_database = {
    "tech": """
    OpenAI ä»Šæ—¥å®£å¸ƒæ¨å‡º GPT-5ï¼Œé€™æ˜¯å…¶æœ€æ–°ä¸€ä»£çš„å¤§å‹èªè¨€æ¨¡å‹ã€‚
    æ–°æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ã€å¤šèªè¨€æ”¯æ´å’Œç¨‹å¼ç¢¼ç”Ÿæˆæ–¹é¢éƒ½æœ‰é¡¯è‘—æå‡ã€‚
    CEO Sam Altman è¡¨ç¤ºï¼Œé€™æ¨™èªŒè‘— AI ç™¼å±•çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚
    è©²æ¨¡å‹å·²é–‹å§‹å‘ä¼æ¥­ç”¨æˆ¶é–‹æ”¾æ¸¬è©¦ã€‚
    """,
    "sports": """
    ä¸–ç•Œæ¯è¶³çƒè³½æ˜¨æ™šçµæŸäº†ç²¾å½©çš„å…«å¼·è³½ã€‚
    æ³•åœ‹éšŠä»¥ 2-1 æ“Šæ•—è‹±æ ¼è˜­éšŠï¼Œæ™‰ç´šå››å¼·ã€‚
    æ¯”è³½åœ¨åŠ æ™‚è³½ä¸­ç”± MbappÃ© æ”»å…¥è‡´å‹çƒã€‚
    é€™æ˜¯æ³•åœ‹éšŠé€£çºŒç¬¬äºŒå±†ä¸–ç•Œæ¯é€²å…¥å››å¼·ã€‚
    """,
    "business": """
    å°ç©é›»å®£å¸ƒå°‡åœ¨æ—¥æœ¬ç†Šæœ¬ç¸£å»ºè¨­ç¬¬ä¸‰åº§æ™¶åœ“å» ã€‚
    é è¨ˆæŠ•è³‡é‡‘é¡è¶…é 200 å„„ç¾å…ƒï¼Œå°‡å‰µé€ è¶…é 3000 å€‹å°±æ¥­æ©Ÿæœƒã€‚
    æ–°å» é è¨ˆæ–¼ 2027 å¹´é–‹å§‹é‡ç”¢ï¼Œä¸»è¦ç”Ÿç”¢ 3 å¥ˆç±³è£½ç¨‹æ™¶ç‰‡ã€‚
    é€™æ˜¯å°ç©é›»æµ·å¤–æ“´å¼µè¨ˆç•«çš„é‡è¦ä¸€æ­¥ã€‚
    """
}


# 2. å®šç¾©å·¥å…· - ä½¿ç”¨ Sampling ä¾†ç¸½çµæ–°è
@mcp.tool()
async def summarize_news(category: str, language: str = "ç¹é«”ä¸­æ–‡") -> str:
    """
    å–å¾—æŒ‡å®šé¡åˆ¥çš„æ–°èä¸¦ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦ã€‚
    
    Args:
        category: æ–°èé¡åˆ¥ (tech, sports, business)
        language: è¼¸å‡ºèªè¨€ (ç¹é«”ä¸­æ–‡, English, æ—¥æœ¬èª)
    
    Returns:
        AI ç”Ÿæˆçš„æ–°èæ‘˜è¦
    """
    if category not in news_database:
        return f"æ‰¾ä¸åˆ°é¡åˆ¥ï¼š{category}ã€‚å¯ç”¨é¡åˆ¥ï¼štech, sports, business"
    
    news_content = news_database[category]
    
    # ğŸ”¥ é€™è£¡ä½¿ç”¨ Sampling - è«‹æ±‚ AI å¹«å¿™ç¸½çµï¼
    ctx = mcp.get_context()
    
    result = await ctx.sample(
        f"""è«‹å°‡ä»¥ä¸‹æ–°èå…§å®¹ç¸½çµç‚º 2-3 å¥è©±çš„ç²¾ç°¡æ‘˜è¦ã€‚
ä½¿ç”¨ {language} è¼¸å‡ºã€‚

æ–°èå…§å®¹ï¼š
{news_content}

è«‹åªè¼¸å‡ºæ‘˜è¦ï¼Œä¸è¦åŠ å…¥ä»»ä½•é¡å¤–èªªæ˜ã€‚""",
        max_tokens=200
    )
    
    return f"ğŸ“° {category.upper()} æ–°èæ‘˜è¦ï¼š\n\n{result.text}"


# 3. å®šç¾©å·¥å…· - ä½¿ç”¨ Sampling ä¾†ç¿»è­¯
@mcp.tool()
async def smart_translate(text: str, target_language: str) -> str:
    """
    ä½¿ç”¨ AI é€²è¡Œæ™ºæ…§ç¿»è­¯ï¼ˆä¿ç•™èªæ°£å’Œé¢¨æ ¼ï¼‰ã€‚
    
    Args:
        text: è¦ç¿»è­¯çš„æ–‡å­—
        target_language: ç›®æ¨™èªè¨€ (English, æ—¥æœ¬èª, í•œêµ­ì–´, ç¹é«”ä¸­æ–‡)
    
    Returns:
        ç¿»è­¯å¾Œçš„æ–‡å­—
    """
    ctx = mcp.get_context()
    
    # ä½¿ç”¨ Sampling è«‹æ±‚ AI ç¿»è­¯
    result = await ctx.sample(
        f"""è«‹å°‡ä»¥ä¸‹æ–‡å­—ç¿»è­¯æˆ {target_language}ã€‚
ä¿æŒåŸæ–‡çš„èªæ°£å’Œé¢¨æ ¼ï¼Œé€²è¡Œè‡ªç„¶çš„ç¿»è­¯è€Œéé€å­—ç¿»è­¯ã€‚

åŸæ–‡ï¼š
{text}

è«‹åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŠ å…¥ä»»ä½•èªªæ˜ã€‚""",
        max_tokens=500
    )
    
    return f"ğŸŒ ç¿»è­¯çµæœ ({target_language})ï¼š\n\n{result.text}"


# 4. å®šç¾©å·¥å…· - ä½¿ç”¨ Sampling ç”Ÿæˆå›æ‡‰å»ºè­°
@mcp.tool()
async def generate_reply_suggestions(message: str, tone: str = "professional") -> str:
    """
    æ ¹æ“šæ”¶åˆ°çš„è¨Šæ¯ï¼Œä½¿ç”¨ AI ç”Ÿæˆå¤šå€‹å›è¦†å»ºè­°ã€‚
    
    Args:
        message: æ”¶åˆ°çš„è¨Šæ¯å…§å®¹
        tone: å›è¦†èªæ°£ (professional, friendly, formal, casual)
    
    Returns:
        ä¸‰å€‹ä¸åŒçš„å›è¦†å»ºè­°
    """
    ctx = mcp.get_context()
    
    result = await ctx.sample(
        f"""ä½ æ”¶åˆ°äº†ä»¥ä¸‹è¨Šæ¯ï¼Œè«‹ç”Ÿæˆ 3 å€‹ä¸åŒçš„å›è¦†å»ºè­°ã€‚
ä½¿ç”¨ {tone} çš„èªæ°£ã€‚

æ”¶åˆ°çš„è¨Šæ¯ï¼š
ã€Œ{message}ã€

è«‹ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š
1. [ç¬¬ä¸€å€‹å›è¦†å»ºè­°]
2. [ç¬¬äºŒå€‹å›è¦†å»ºè­°]  
3. [ç¬¬ä¸‰å€‹å›è¦†å»ºè­°]""",
        max_tokens=400
    )
    
    return f"ğŸ’¬ å›è¦†å»ºè­°ï¼ˆ{tone} èªæ°£ï¼‰ï¼š\n\n{result.text}"


# å•Ÿå‹• Server
if __name__ == "__main__":
    mcp.run(transport="sse")
```

---

## Step 2: é‹è¡Œ Sampling Server

ç¢ºä¿è™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•ï¼Œç„¶å¾ŒåŸ·è¡Œï¼š

```bash
python sampling_server.py
```

ä½ æ‡‰è©²æœƒçœ‹åˆ°ï¼š
```
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8080
```

---

## Step 3: æ¸¬è©¦ Sampling åŠŸèƒ½ ğŸ§ª

### ä½¿ç”¨ MCP Inspector

1. **é–‹å•Ÿæ–°çµ‚ç«¯**ï¼Œå•Ÿå‹• Inspectorï¼š
```bash
npx @modelcontextprotocol/inspector
```

2. **é€£æ¥åˆ° Server**ï¼š
   - é¸æ“‡ **SSE** é€£æ¥æ–¹å¼
   - è¼¸å…¥ URLï¼š`http://localhost:8080/sse`
   - é»æ“Š **Connect**

3. **æ¸¬è©¦å·¥å…·**ï¼š

| å·¥å…· | æ¸¬è©¦åƒæ•¸ | é æœŸçµæœ |
|------|----------|----------|
| `summarize_news` | category: "tech" | AI ç”Ÿæˆçš„ç§‘æŠ€æ–°èæ‘˜è¦ |
| `smart_translate` | text: "ä½ å¥½ä¸–ç•Œ", target_language: "English" | ç¿»è­¯çµæœ |
| `generate_reply_suggestions` | message: "æ˜å¤©çš„æœƒè­°å¯ä»¥æ”¹æœŸå—ï¼Ÿ" | 3 å€‹å›è¦†å»ºè­° |

---
